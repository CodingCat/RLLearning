{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all essential libraries\n",
    "import sys \n",
    "import numpy as np # linear algebra\n",
    "from scipy.stats import randint\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "## Deep-learing:\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/household_power_consumption.txt', sep=';', \n",
    "                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n",
    "                 low_memory=False, na_values=['nan','?'], index_col='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[data.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def get_acf(data,lags): \n",
    "    frame = []\n",
    "    for i in range(lags+1):\n",
    "        frame.append(data.apply(lambda col: col.autocorr(i), axis=0))\n",
    "    return pd.DataFrame(frame).plot.line()\n",
    "get_acf(data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in missing values using linear interpolation with limit to 10 using autocorelation plot\n",
    "clean_data = data.interpolate(method = 'linear', axis = 0, limit = 10)\n",
    "\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize=(20, 15))\n",
    "for counter in range(1,len(clean_data.columns)):\n",
    "    plt.subplot(len(clean_data.columns), 1, i)\n",
    "    plt.plot(clean_data.resample('D').mean().values[:, counter], color = 'blue')\n",
    "    plt.title(clean_data.columns[counter], y=0.8, loc='right')\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # used for plot interactive graph. \n",
    "\n",
    "#check corelation matrices for minute, hour and day\n",
    "#minute\n",
    "corr_min = clean_data.corr()\n",
    "#hour\n",
    "corr_hour = pd.DataFrame(clean_data.resample('H').mean().values).corr()\n",
    "#day\n",
    "corr_day = pd.DataFrame(clean_data.resample('D').mean().values).corr()\n",
    "cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "#Plot Minute\n",
    "corr_min.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "    .set_table_styles(magnify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_hour.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "    .set_table_styles(magnify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_day.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "    .set_table_styles(magnify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit: Adopted from https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    dff = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(dff.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(dff.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = clean_data.resample('H').mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed_data = series_to_supervised(resampled_data, 3, 1)\n",
    "print(reframed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed_data.drop(reframed_data.columns[-6:], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reframed_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed_data.values\n",
    "#The logic is to have 500 days worth of training data. this could also be a hyperparameter that can be tuned.\n",
    "train_index = 800*24 \n",
    "train = values[:train_index, :]\n",
    "test = values[train_index:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 3, int(train_X.shape[1]/3)))\n",
    "test_X = test_X.reshape((test_X.shape[0], 3, int(test_X.shape[1]/3)))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n",
    "# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, return_sequences=True, input_shape=((3, train_X.shape[2]))))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False, input_shape=((3, 200))))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=20, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(train_X, verbose=0)\n",
    "rmse_train = np.sqrt(mean_squared_error(train_y, yhat))\n",
    "yhat = model.predict(test_X, verbose=0)\n",
    "rmse_test = np.sqrt(mean_squared_error(test_y, yhat))\n",
    "print('Train RMSE: %.3f' % rmse_train)\n",
    "print('Test RMSE: %.3f' % rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_for_xgb = model\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model_for_xgb.input,\n",
    "                                 outputs=model_for_xgb.get_layer(index=1).output)\n",
    "\n",
    "intermediate_train = intermediate_layer_model.predict(train_X)\n",
    "intermediate_test = intermediate_layer_model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(intermediate_train, train_y)\n",
    "param = {'max_depth': 4, 'objective':'reg:linear', 'eta': 0.1, 'tree_method': 'hist', 'nthread': 4,\n",
    "         'max_bin': 256, \"grow_policy\": \"lossguide\", \"max_leaves\": 32}\n",
    "bst = xgb.train(param, dtrain, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "predictions = bst.predict(xgb.DMatrix(intermediate_train, train_y))\n",
    "rmse_train = np.sqrt(mean_squared_error(train_y, predictions))\n",
    "print(\"Train RMSE: %.3f\" % rmse_train)\n",
    "predictions = bst.predict(xgb.DMatrix(intermediate_test, test_y))\n",
    "rmse_test = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "print(\"Test RMSE: %.3f\" % rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.to_graphviz(bst, num_trees=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
